---
title: "Metabolon Metabolite Data QC Report"
author: "L.J.Corbin"
date: "31/03/2022"
---

```{r initialise, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}

# SET UP - update function script and data files directories in this section accordingly

# read in parameter file
pfile = read.table("U:/Scripts/02.0_metab_qc_param_file.R", sep = "=",as.is=T)

# set parameters
project <- as.character( pfile[1,2] )
dataset <- as.character( pfile[2,2] )
processseddata_dir <- as.character( pfile[3,2] )
scripts_dir <- as.character( pfile[4,2] )
rawdata_dir <- as.character( pfile[5,2] )
imputed_flag <- as.character( pfile[6,2] )
output_dir <- as.character( pfile[7,2] )
extra_fails <- as.character( pfile[8,2] )
extra_fails <- strsplit(extra_fails,split="_")[[1]]
min_sample <- as.numeric( pfile[9,2] )

## call R script containing functions
source(paste0(scripts_dir,"02.0_qc_functions.R"))

```


```{r read_in_data, include=FALSE}

## read in data
## original scale version
orig_data <- readRDS(paste0(processseddata_dir,"OrigScale_raw_data.rds"))
orig_feature <- readRDS(paste0(processseddata_dir,"OrigScale_feature_metadata.rds"))
orig_sample <- readRDS(paste0(processseddata_dir,"OrigScale_sample_metadata.rds"))

```

## Data overview

Number of samples in OrigScale file: `r nrow(orig_sample)`  
  Number of features in OrigScale file: `r nrow(orig_feature)`  

## Power exploration for case/control analysis

```{r power_exploration_binary, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }
   
# set N equal to no. in sample (need to divide by two as repeated measures here)
N = ceiling(ncol(orig_data)/2)

# calculate power for different scenarios
run_parameters <- expand.grid(N = N*(seq(0.10,1,0.10)),
                             effect = c(0.1,0.2,0.3,0.4,0.5,0.75,1.0),
                             alpha = 0.05)

eval_power <- lapply(1:nrow(run_parameters), function(x){
    eval.power.binary(N= run_parameters[x,1], effect = run_parameters[x,2], alpha = run_parameters[x,3])
})

eval_power_out <- as.data.frame(do.call(rbind, eval_power))

# plot results
plot_data_reduce <- melt(as.data.table(eval_power_out), id=c("N","N_case", "N_control", "effect", "alpha"))
plot_data_reduce$effect <- as.factor(plot_data_reduce$effect)

ggplot(plot_data_reduce,aes(x=N, y=value, group=effect,color=effect)) +
    geom_line(data=plot_data_reduce) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, vjust=1, hjust=1)) +
    geom_hline(yintercept=0.8) +
    ylab("Power") +
    xlab(bquote('Total sample size')) +
    scale_x_continuous(breaks=seq(0, N, 20)) +
    scale_y_continuous(breaks=seq(0,1,0.2)) +
    expand_limits(y=0) +
    ggtitle(bquote(atop('Estimated power at a range of standardized effect sizes'))) +
    theme(plot.title = element_text(hjust = 0.5))

# establish min. sample size for detecting a 1SD effect
min_sample_for_power <- ceiling(2*(pwr.t.test(d = 1, sig.level = 0.05, power = 0.8, type="two.sample")$n))

```

The minimum sample size required for achieving 80% power assuming a standardised effect size of 1, and an equal split between intervention/control groups, is: `r min_sample_for_power`

## Power exploration for continuous outcome analysis

```{r power_exploration_continuous, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5}

# set N equal to no. in sample (need to divide by two as repeated measures here)
N = ceiling(ncol(orig_data)/2)

# calculate power for different scenarios
run_parameters <- expand.grid(N = N*(seq(0.10,1,0.10)),
                             n_coeff = 1,
                             effect = c(0.01,0.025,0.05,0.075,0.1,0.2,0.3),
                             alpha = 0.05)

eval_power <- lapply(1:nrow(run_parameters), function(x){
    eval.power.cont(N= run_parameters[x,1], n_coeff = run_parameters[x,2], effect = run_parameters[x,3], alpha = run_parameters[x,4])
})

eval_power_out <- as.data.frame(do.call(rbind, eval_power))

# plot results
plot_data_reduce <- melt(as.data.table(eval_power_out), id=c("N", "n_coeff", "effect", "alpha"))
plot_data_reduce$effect <- as.factor(plot_data_reduce$effect)

ggplot(plot_data_reduce,aes(x=N, y=value, group=effect,color=effect)) +
    geom_line(data=plot_data_reduce) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 45, vjust=1, hjust=1)) +
    geom_hline(yintercept=0.8) +
    ylab("Power") +
    xlab(bquote('Total sample size')) +
    scale_x_continuous(breaks=seq(0, N, 200)) +
    scale_y_continuous(breaks=seq(0,1,0.2)) +
    expand_limits(y=0) +
    ggtitle(bquote(atop('Estimated power at a range of standardized effect sizes'))) +
    theme(plot.title = element_text(hjust = 0.5))


```

## Summarise missingness (pre filtering)

```{r missingness, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }

# summarise missingness across samples and features
sample_missing <- sample.missing(orig_data)
a = summary(sample_missing)
print("Summary of sample missingness:")
a

feature_missing <- feature.missing(orig_data)
b = summary(feature_missing)
print("Summary of feature missingness:")
b

# identify features with zero missing for use in PCA
features_no_missing <- names(which(feature_missing==0))

# generate histograms
# by sample
hist(sample_missing, main="OrigScale - Distribution of by sample missingness", cex.main=1.0, xlab="")
abline(v=a[4],col="red")
title(sub=paste0("min.=",round(a[1],digits=3),", max.=",round(a[6],digits=3) ) )
# by feature
hist(feature_missing, main="OrigScale - Distribution of by feature missingness", cex.main=1.0, xlab="")
abline(v=b[4],col="red")
title(sub=paste0("min.=",round(b[1],digits=3),", max.=",round(b[6],digits=3) ) )

# add summary stats to sample file
sample_missing_dat <- as.data.frame(sample_missing)
sample_missing_dat$sample.name <- row.names(sample_missing_dat)
orig_sample$missingness_prefilter <- sample_missing_dat$sample_missing[match(orig_sample$sample.name,sample_missing_dat$sample.name)]

# add summary stats to feature file
feature_missing_dat <- as.data.frame(feature_missing)
feature_missing_dat$comp.id <- row.names(feature_missing_dat)
orig_feature$missingness_prefilter <- feature_missing_dat$feature_missing[match(orig_feature$compid_to_match,feature_missing_dat$comp.id)]

```

There are `r length(features_no_missing)` features with no data missing.

### Example sample numbers at different missingness rates

If feature has 10% missingness, data is available for: `r (nrow(orig_sample)*0.9)` samples.  
  If feature has 20% missingness, data is available for: `r (nrow(orig_sample)*0.8)` samples.  
  If feature has 30% missingness, data is available for: `r (nrow(orig_sample)*0.7)` samples.  
  If feature has 40% missingness, data is available for: `r (nrow(orig_sample)*0.6)` samples.  
  If feature has 50% missingness, data is available for: `r (nrow(orig_sample)*0.5)` samples.

## Summarise outliers (pre filtering)

### Outliers are defined both as values greater than or less than 5SD from the mean and as values outside the 1st/99th percentile.

```{r outliers_pre_filtering, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }


# run outlier summary based on 5SD

sd_outlier_filename <- paste0(output_dir,dataset,"_SD_outlier_detection_pre_filtering.pdf")

#run outlier function
sd_outlier_summary <- outlier.summary(dtst=orig_data, scaling_flag=1, pdf_filename=sd_outlier_filename, nsd=5, cut=0.01, method="sd")

sd_outlier_results_prefilter <- sd_outlier_summary[[1]]
sd_data_winsorised_prefilter <- as.data.frame(t(sd_outlier_summary[[2]]))

# summarise outliers across samples and features
sample_sd_outlier_summary <- rowSums(sd_outlier_results_prefilter, na.rm = T, dims = 1)
a = summary(sample_sd_outlier_summary)
print("Summary of SD outliers by sample (pre filtering):")
a

feature_sd_outlier_summary <- colSums (sd_outlier_results_prefilter, na.rm = T, dims = 1)
b = summary(feature_sd_outlier_summary)
print("Summary of SD outliers by feature (pre filtering):")
b

# add outlier stats to feature file
feature_sd_outlier_dat <- as.data.frame(feature_sd_outlier_summary)
feature_sd_outlier_dat$comp.id <- row.names(feature_sd_outlier_dat)
orig_feature$sd_outlier_prefilter <- feature_sd_outlier_dat$feature_sd_outlier_summary[match(orig_feature$compid_to_match,feature_sd_outlier_dat$comp.id)]

##########

# run outlier summary based on percentile (0.01)

pc_outlier_filename <- paste0(output_dir,dataset,"_pc_outlier_detection_pre_filtering.pdf")

#run outlier function
pc_outlier_summary <- outlier.summary(dtst=orig_data, scaling_flag=1, pdf_filename=pc_outlier_filename, nsd=5, cut=0.01, method="percentile")

pc_outlier_results_prefilter <- pc_outlier_summary[[1]]
pc_data_winsorised_prefilter <- as.data.frame(t(pc_outlier_summary[[2]]))

# summarise outliers across samples and features
sample_pc_outlier_summary <- rowSums(pc_outlier_results_prefilter, na.rm = T, dims = 1)
a = summary(sample_pc_outlier_summary)
print("Summary of percentile outliers by sample (pre filtering):")
a

feature_pc_outlier_summary <- colSums (pc_outlier_results_prefilter, na.rm = T, dims = 1)
b = summary(feature_pc_outlier_summary)
print("Summary of percentile outliers by feature (pre filtering):")
b

# add outlier stats to feature file
feature_pc_outlier_dat <- as.data.frame(feature_pc_outlier_summary)
feature_pc_outlier_dat$comp.id <- row.names(feature_pc_outlier_dat)
orig_feature$pc_outlier_prefilter <- feature_pc_outlier_dat$feature_pc_outlier_summary[match(orig_feature$compid_to_match,feature_pc_outlier_dat$comp.id)]

```


## Apply missingness thresholds to data and re-summarise

Sample missingness applied using all features except those classified as xenobiotics.  
  Samples excluded if more than 20% of features are missing.  
  Features excluded if they are measured in less than `r min_sample` samples.

```{r missingness_apply, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }

## create a subset of data without the xenobiotics
xeno_features <- orig_feature$compid_to_match[orig_feature$super.pathway == "Xenobiotics" & !is.na(orig_feature$super.pathway)]
noxeno_data <- orig_data[!rownames(orig_data) %in% xeno_features,]

## identify samples with high missingness (for exclusion)
samples_to_exclude <- sample.missing.exclusions(noxeno_data, threshold = 0.2) 

## apply sample exclusions
if(length(samples_to_exclude) > 0) {
  orig_data_clean_1 <- sample.exclude(orig_data,exclusions = samples_to_exclude)
} else {
  orig_data_clean_1 <- orig_data
}

## identify features with high missingness (for exclusion) AFTER sample exclusions
## calculate % that is equivalent to n=min_sample
feature_thresh <- 1-((min_sample)/ncol(orig_data_clean_1))
features_to_exclude <- feature.missing.exclusions(orig_data_clean_1, threshold = feature_thresh) 

## create version of data with features removed based on missingness
if(length(features_to_exclude) > 0) {
    orig_data_clean <- feature.exclude(orig_data_clean_1,exclusions = features_to_exclude)
  } else {
    orig_data_clean <- data_clean_1
}

## add flags to sample metadata
orig_sample$high_missingness_flag <- 0
orig_sample[which(orig_sample$sample.name %in% samples_to_exclude),c("high_missingness_flag")] <- 1

## add flags to feature metadata
orig_feature$high_missingness_flag <- 0
orig_feature[which(orig_feature$comp.id %in% features_to_exclude),c("high_missingness_flag")] <- 1

```

Number of sample exclusions based on >20% missingness: `r length(samples_to_exclude)`  
  Sample missingness is calculated after exclusion of `r length(xeno_features)` xenobiotics, leaving `r nrow(noxeno_data)` features.
  Number of feature exclusions based on >`r feature_thresh` missingness: `r length(features_to_exclude)`

```{r missingness_post_apply, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }

# summarise missingness across samples and features
sample_missing <- sample.missing(orig_data_clean)
a = summary(sample_missing)
print("Summary of sample missingness after cleaning:")
a

feature_missing <- feature.missing(orig_data_clean)
b = summary(feature_missing)
print("Summary of feature missingness after cleaning:")
b

# generate histograms
# by sample
hist(sample_missing, main="OrigScale - Distribution of by sample missingness after filtering", cex.main=1.0, xlab="")
abline(v=a[4],col="red")
title(sub=paste0("min.=",round(a[1],digits=3),", max.=",round(a[6],digits=3) ) )
# by feature
hist(feature_missing, main="OrigScale - Distribution of by feature missingness after filtering", cex.main=1.0, xlab="")
abline(v=b[4],col="red")
title(sub=paste0("min.=",round(b[1],digits=3),", max.=",round(b[6],digits=3) ) )

```



## Total peak area assessment (by sample) (based on filtered OrigScale data)


```{r tpa, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }

# calculate tpa
sample_tpa <- sample.tpa(orig_data_clean[features_no_missing,])
c = summary(sample_tpa)
print("Summary of total peak area:")
c

# work out exclusions based on outliers plus/minus 'n' standard deviations from the mean
# 3sd
outlier_info_3 <- outliers(sample_tpa,3)
columns_to_exclude_on_tpa_3 <- outlier_info_3[[1]]
samples_to_exclude_on_tpa_3 <- names(orig_data_clean)[columns_to_exclude_on_tpa_3]

# 5sd
outlier_info_5 <- outliers(sample_tpa,5)
columns_to_exclude_on_tpa_5 <- outlier_info_5[[1]]
samples_to_exclude_on_tpa_5 <- names(orig_data_clean)[columns_to_exclude_on_tpa_5]

# generate histogram
hist(sample_tpa, main="OrigScale - Distribution of total peak area (across samples)", xlab="Sample total peak area")
abline(v=outlier_info_3[[3]],col="orange")
abline(v=outlier_info_3[[2]],col="orange")
abline(v=outlier_info_5[[3]],col="red")
abline(v=outlier_info_5[[2]],col="red")

## apply sample exclusions
if(length(samples_to_exclude_on_tpa_5) > 0) {
  orig_data_clean <- sample.exclude(orig_data_clean,exclusions = samples_to_exclude_on_tpa_5)
} else {
  orig_data_clean <- orig_data_clean
}

## add flags to sample metadata
orig_sample$tpa_3SD_flag <- 0
orig_sample[which(orig_sample$sample.name %in% samples_to_exclude_on_tpa_3),c("tpa_3SD_flag")] <- 1

orig_sample$tpa_5SD_flag <- 0
orig_sample[which(orig_sample$sample.name %in% samples_to_exclude_on_tpa_5),c("tpa_5SD_flag")] <- 1

```

Number of sample exclusions based on total peak area (+/- 3sd from the mean): `r length(samples_to_exclude_on_tpa_3)`  
  Number of sample exclusions based on total peak area (+/- 5sd from the mean) (current criteria): `r length(samples_to_exclude_on_tpa_5)`  


## Plot principal components - check for sample outliers

### Identify independent features based on Spearman's correlation

```{r correlation, echo=F, quote=F, comment=NA, fig.width=20, fig.height=10}

### identify subset of 'independent' features 

# make tree
feature_missing <- feature.missing(dtst=orig_data_clean)
features_grt80 <- names(which(feature_missing < 0.20))
#SpearTree <- make.tree(dtst=orig_data_clean[features_no_missing,],cor_method="spearman",hclust_method="complete")
SpearTree <- make.tree(dtst=orig_data_clean[features_grt80,] ,cor_method="spearman", hclust_method="complete")

# restrict based on cut off
k = cutree(SpearTree, h = 0.20)

# restrict so as to keep the one feature in each cluster (prioritise one with least missing)
k_group = table(k)
# strictly independent features (i.e. one feature classes)
w = which(k_group == 1); ind_k = names( k_group[w] ); w = which(k %in% ind_k)
ind = names(k)[w]
# for remaining classes, identify feature with least missingness
# identify representative feature from each class with more than one feature in it
w = which(k_group > 1); k_group = names( k_group[w] )
ind2 = sapply(k_group, function(x){
  w = which(k %in% x); n=names(k[w])
  o = sort(feature_missing[n], decreasing = FALSE)
  out = names(o)[1]
})
# join the two lists
independent_features = paste( c(ind, ind2))

# restrict so as to keep the first feature in each cluster
data_subset <- orig_data_clean[independent_features,]

## plot tree
plot(SpearTree, hang = -1, cex = 0.5, main = "Spearman Cluster Dendrogram with cut height of 0.20", col = "royalblue", lwd = 2)
rect.hclust( SpearTree , h = 0.20, border="red")


```

Total number of features with <20% missingness (used for identifying independent features): `r length(k)`  
  Number of independent features used for PCA (based on a tree cut height of 0.20): `r length(unique(k))`  

### Generate PCs

```{r pca_round1, echo=F, quote=F, comment=NA, fig.width=20, fig.height=10}

## calculate PCs
# scale and centre
scaled_data_subset <- scale(t(data_subset))
# run probabilistic pca (allowing missingness)
pca <- ppca(scaled_data_subset,seed=12345,nPcs=20)

# extract variance explained
print("Round 1 PCA - variance explained:")
round( (summary(pca)[1,]), d=2)

## plot PCs
pairs( pca@scores[,1:5], main="Round 1 PCA")

## identify and exclude outliers based on PC1 & PC2
pc1_outliers_round1 <- pc.outliers(pca@scores[,1], nsd=5)
pc2_outliers_round1 <- pc.outliers(pca@scores[,2], nsd=5)

exclude_round1 <- c(pc1_outliers_round1[[1]], pc2_outliers_round1[[1]])
exclude_round1_id <- names(data_subset)[c(exclude_round1)]

# make new data set if outliers identified
if (length(exclude_round1) > 0) {
  data_subset_v1 <- data_subset[,-exclude_round1]
} else {
  data_subset_v1 <- data_subset
}

```

PCA Round 1 - Number of samples to exclude based on PC1 (+/- 5sd from the mean): `r length(pc1_outliers_round1[[1]])`  
  PCA Round 1 - Number of samples to exclude based on PC2 (+/- 5sd from the mean): `r length(pc2_outliers_round1[[1]])`  

```{r pca_round2, echo=F, quote=F, comment=NA, fig.width=20, fig.height=10}

# repeat pca if outliers identified in first round made
if (length(exclude_round1) > 0) {

  ## calculate PCs
  # scale and centre
  scaled_data_subset <- scale(t(data_subset_v1))
  # run probabilistic pca (allowing missingness)
  pca <- ppca(scaled_data_subset,seed=12345,nPcs=20)

  # extract variance explained
  print("Round 2 PCA - variance explained:")
  round( (summary(pca)[1,]), d=2)

  ## plot PCs
  pairs( pca@scores[,1:5], main="Round 2 PCA")

  ## identify and exclude outliers based on PC1 & PC2
  pc1_outliers_round2 <- pc.outliers(pca@scores[,1], nsd=5)
  pc2_outliers_round2 <- pc.outliers(pca@scores[,2], nsd=5); 
  
  exclude_round2 <- c(pc1_outliers_round2[[1]], pc2_outliers_round2[[1]])
  exclude_round2_id <- names(data_subset)[c(exclude_round2)]

  # make new data set if outliers identified
  if (length(exclude_round2) > 0) {
    data_subset_v2 <- data_subset_v1[,-exclude_round2]
  } else {
    data_subset_v2 <- data_subset_v1
  }
} else {
  data_subset_v2 <- data_subset_v1
  pc1_outliers_round2 <- NULL
  pc2_outliers_round2 <- NULL
  exclude_round2_id <- NULL
  print("No PCA outliers identified. Second round of PCA not needed")
}

## add flags to sample metadata
orig_sample$pca1_flag <- 0
orig_sample[which(orig_sample$sample.name %in% exclude_round1_id),c("pca1_flag")] <- 1

orig_sample$pca2_flag <- 0
orig_sample[which(orig_sample$sample.name %in% exclude_round2_id),c("pca2_flag")] <- 1

```

PCA Round 2 - Number of samples to exclude based on PC1 (+/- 5sd from the mean): `r length(pc1_outliers_round2[[1]])`  
  PCA Round 2 - Number of samples to exclude based on PC2 (+/- 5sd from the mean): `r length(pc2_outliers_round2[[1]])`


## Prepare QC'd datasets

```{r write_out, echo=F, quote=F, comment=NA, fig.width=20, fig.height=10}

# make lists of samples and features to keep
samples_to_keep <- names(data_subset_v2)
features_to_keep <- rownames(orig_data_clean)

# remove samples that failed Nightingale QC
samples_to_keep <- samples_to_keep[!samples_to_keep %in% extra_fails]

## add flags to sample metadata
orig_sample$nmr_quality_flag <- 0
orig_sample[which(orig_sample$sample.name %in% (orig_sample$sample.name[which(orig_sample$client.identifier == extra_fails)])),c("nmr_quality_flag")] <- 1

# extract from data files
orig_data_out <- orig_data[features_to_keep,samples_to_keep]
saveRDS(orig_data_out, file = paste0(paste0(processseddata_dir,"OrigScale_cleaned_data.rds")))

# extract from sample metadata
orig_sample_out <- orig_sample[orig_sample$sample.name %in% samples_to_keep,]
saveRDS(orig_sample_out, file = paste0(paste0(processseddata_dir,"OrigScale_cleaned_sample_metadata.rds")))

# extract from feature metadata
orig_feature_out <- orig_feature[orig_feature$compid_to_match %in% features_to_keep,]
saveRDS(orig_feature_out, file = paste0(paste0(processseddata_dir,"OrigScale_cleaned_feature_metadata.rds")))


```

No. of samples excluded due to failing Nightingale QC: `r length(orig_sample$sample.name[which(orig_sample$sample.name %in% extra_fails)])`

## Data overview post QC

The QC'd data files contain `r length(samples_to_keep)` samples and `r length(features_to_keep)` features.  

## Check distributions (based on cleaned OrigScale data)

```{r origscale_dist, echo=F, quote=F, comment=NA, fig.width=20, fig.height=10 }

## calculate normality statistics from Shapiro test
normality_test_orig <- as.data.frame(t(apply(orig_data_out, 1, normality.test, max_missing = (ncol(orig_data_out)-100), min_unique = 100)))
names(normality_test_orig)[1] <- "shapiro_w"
names(normality_test_orig)[2] <- "shapiro_p"

## calculate proportion normal by p-value and w statistic
prop_norm_w <- proportion.normal.w(normality_test_orig,threshold = 0.95)
prop_norm_p <- proportion.normal.p(normality_test_orig,threshold = 0.01)

## identify least and most normal
least_normal <- least.normal(normality_test_orig,9)
most_normal <- most.normal(normality_test_orig,9)

## generate histograms
## least normal
par(mfrow=c(3,3),oma=c(0,0,3,0),mar=c(4,4,4,4))
for (i in 1:9){
  temp <- as.numeric(orig_data_out[least_normal[i],])
  hist(temp/1000000, main=paste0("CompID: ",least_normal[i]), cex.main=1.0, xlab="OrigScale/10^6")
}
mtext("OrigScale - Least normal distributions",outer=T,side = 3, cex=1.5)

## most normal
par(mfrow=c(3,3),oma=c(0,0,3,0),mar=c(4,4,4,4))
for (i in 1:9){
  temp <- as.numeric(orig_data_out[most_normal[i],])
  hist(temp/1000000, main=paste0("CompID: ",most_normal[i]), cex.main=1.0, xlab="OrigScale/10^6")
  title(sub=paste(most_normal[i]))
}
mtext("OrigScale - Most normal distributions",outer=T,side = 3, cex=1.5)

# add normality stats to feature file
normality_test_orig$comp.id <- row.names(normality_test_orig)
orig_feature$shapiro_w_postfilter <- normality_test_orig$shapiro_w[match(orig_feature$compid_to_match,feature_missing_dat$comp.id)]
orig_feature$shapiro_p_postfilter <- normality_test_orig$shapiro_p[match(orig_feature$compid_to_match,feature_missing_dat$comp.id)]

```

Proportion of features with a normal distribution (w>0.95): `r prop_norm_w`  
  Proportion of features with a normal distribution (p>0.01): `r prop_norm_p`  

## Summarise missingness (post QC)

```{r missingness_post_QC, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }

# summarise missingness across samples and features
sample_missing <- sample.missing(orig_data_out)
a = summary(sample_missing)
print("Summary of sample missingness:")
a

feature_missing <- feature.missing(orig_data_out)
b = summary(feature_missing)
print("Summary of feature missingness:")
b

# identify features with zero missing for use in PCA
features_no_missing <- names(which(feature_missing==0))

# generate histograms
# by sample
hist(sample_missing, main="OrigScale - Distribution of by sample missingness after QC", cex.main=1.0, xlab="")
abline(v=a[4],col="red")
title(sub=paste0("min.=",round(a[1],digits=3),", max.=",round(a[6],digits=3) ) )
# by feature
hist(feature_missing, main="OrigScale - Distribution of by feature missingness after QC", cex.main=1.0, xlab="")
abline(v=b[4],col="red")
title(sub=paste0("min.=",round(b[1],digits=3),", max.=",round(b[6],digits=3) ) )

# add summary stats to sample file
sample_missing_dat <- as.data.frame(sample_missing)
sample_missing_dat$sample.name <- row.names(sample_missing_dat)
orig_sample$missingness_postQC <- sample_missing_dat$sample_missing[match(orig_sample$sample.name,sample_missing_dat$sample.name)]

# add summary stats to feature file
feature_missing_dat <- as.data.frame(feature_missing)
feature_missing_dat$comp.id <- row.names(feature_missing_dat)
orig_feature$missingness_postQC <- feature_missing_dat$feature_missing[match(orig_feature$compid_to_match,feature_missing_dat$comp.id)]

```

## Summarise outliers (post QC)

### Outliers are defined both as values greater than or less than 5SD from the mean and as values outside the 1st/99th percentile.

```{r outliers_post_QC, echo=F, quote=F, comment=NA, fig.width=10, fig.height=5 }


# run outlier summary based on 5SD

sd_outlier_filename <- paste0(output_dir,dataset,"_SD_outlier_detection_post_QC.pdf")

#run outlier function
sd_outlier_summary <- outlier.summary(dtst=orig_data_out, scaling_flag=1, pdf_filename=sd_outlier_filename, nsd=5, cut=0.01, method="sd")

sd_outlier_results_postQC <- sd_outlier_summary[[1]]
sd_data_winsorised_postQC <- as.data.frame(t(sd_outlier_summary[[2]]))

# summarise outliers across samples and features
sample_sd_outlier_summary <- rowSums(sd_outlier_results_postQC, na.rm = T, dims = 1)
a = summary(sample_sd_outlier_summary)
print("Summary of SD outliers by sample (post filtering):")
a

feature_sd_outlier_summary <- colSums (sd_outlier_results_postQC, na.rm = T, dims = 1)
b = summary(feature_sd_outlier_summary)
print("Summary of SD outliers by feature (post filtering):")
b

# add outlier stats to feature file
feature_sd_outlier_dat <- as.data.frame(feature_sd_outlier_summary)
feature_sd_outlier_dat$comp.id <- row.names(feature_sd_outlier_dat)
orig_feature$sd_outlier_postQC <- feature_sd_outlier_dat$feature_sd_outlier_summary[match(orig_feature$compid_to_match,feature_sd_outlier_dat$comp.id)]

##########

# run outlier summary based on percentile (0.01)

pc_outlier_filename <- paste0(output_dir,dataset,"_pc_outlier_detection_post_QC.pdf")

#run outlier function
pc_outlier_summary <- outlier.summary(dtst=orig_data_out, scaling_flag=1, pdf_filename=pc_outlier_filename, nsd=5, cut=0.01, method="percentile")

pc_outlier_results_postQC <- pc_outlier_summary[[1]]
pc_data_winsorised_postQC <- as.data.frame(t(pc_outlier_summary[[2]]))

# summarise outliers across samples and features
sample_pc_outlier_summary <- rowSums(pc_outlier_results_postQC, na.rm = T, dims = 1)
a = summary(sample_pc_outlier_summary)
print("Summary of percentile outliers by sample (post QC):")
a

feature_pc_outlier_summary <- colSums (pc_outlier_results_postQC, na.rm = T, dims = 1)
b = summary(feature_pc_outlier_summary)
print("Summary of percentile outliers by feature (postQC):")
b

# add outlier stats to feature file
feature_pc_outlier_dat <- as.data.frame(feature_pc_outlier_summary)
feature_pc_outlier_dat$comp.id <- row.names(feature_pc_outlier_dat)
orig_feature$pc_outlier_postQC <- feature_pc_outlier_dat$feature_pc_outlier_summary[match(orig_feature$compid_to_match,feature_pc_outlier_dat$comp.id)]

```


```{r write_new_metadata, echo=F, quote=F, comment=NA, }

## add QC pass/fail flag to sample metadata
orig_sample$qc_flag <- "fail"
orig_sample[which(orig_sample$sample.name %in% samples_to_keep),c("qc_flag")] <- "pass"

## add pass/fail flag to feature metadata
orig_feature$qc_flag <- "fail"
orig_feature[which(orig_feature$compid_to_match %in% features_to_keep),c("qc_flag")] <- "pass"

saveRDS(orig_sample, file = paste0(paste0(processseddata_dir,"OrigScale_sample_metadata_with_QCstats.rds")))
saveRDS(orig_feature, file = paste0(paste0(processseddata_dir,"OrigScale_feature_metadata_with_QCstats.rds")))


```


```{r sessioninfo}
sessionInfo()
```